{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Force CPU usage\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "num_cores = 8\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : 1, 'GPU' : 0})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For plot\n",
    "\n",
    "def prepare_standardplot(title, xlabel):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(title)\n",
    "    ax1.set_ylabel('categorical cross entropy')\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_yscale('log')\n",
    "    ax2.set_ylabel('accuracy [% correct]')\n",
    "    ax2.set_xlabel(xlabel)\n",
    "    return fig, ax1, ax2\n",
    "\n",
    "def finalize_standardplot(fig, ax1, ax2):\n",
    "    ax1handles, ax1labels = ax1.get_legend_handles_labels()\n",
    "    if len(ax1labels) > 0:\n",
    "        ax1.legend(ax1handles, ax1labels)\n",
    "    ax2handles, ax2labels = ax2.get_legend_handles_labels()\n",
    "    if len(ax2labels) > 0:\n",
    "        ax2.legend(ax2handles, ax2labels)\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "\n",
    "def plot_history(history, title):\n",
    "    fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "    ax1.plot(history.history['loss'], label = \"training\")\n",
    "    ax2.plot(history.history['binary_accuracy'], label = \"training\")\n",
    "    finalize_standardplot(fig, ax1, ax2)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/bitstampUSD_1-min_data_2012-01-01_to_2018-01-08.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df= df.drop('Timestamp',1)\n",
    "df[['Open','High','Low','Close']] = df[['Open','High','Low','Close']].apply(lambda x: np.log(x))\n",
    "df = (df-df.mean())/df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume_(BTC)</th>\n",
       "      <th>Volume_(Currency)</th>\n",
       "      <th>Weighted_Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.161057e+06</td>\n",
       "      <td>3.161057e+06</td>\n",
       "      <td>3.161057e+06</td>\n",
       "      <td>3.161057e+06</td>\n",
       "      <td>3.161057e+06</td>\n",
       "      <td>3.161057e+06</td>\n",
       "      <td>3.161057e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.015734e-13</td>\n",
       "      <td>4.370263e-13</td>\n",
       "      <td>3.686399e-13</td>\n",
       "      <td>6.005367e-13</td>\n",
       "      <td>-5.074202e-14</td>\n",
       "      <td>8.689802e-16</td>\n",
       "      <td>-3.917382e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.150525e+00</td>\n",
       "      <td>-2.150622e+00</td>\n",
       "      <td>-2.634462e+00</td>\n",
       "      <td>-2.634494e+00</td>\n",
       "      <td>-3.099179e-01</td>\n",
       "      <td>-1.844678e-01</td>\n",
       "      <td>-4.246458e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-3.932670e-01</td>\n",
       "      <td>-3.934676e-01</td>\n",
       "      <td>-3.933023e-01</td>\n",
       "      <td>-3.932387e-01</td>\n",
       "      <td>-2.974788e-01</td>\n",
       "      <td>-1.832231e-01</td>\n",
       "      <td>-3.772309e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.458171e-01</td>\n",
       "      <td>2.458036e-01</td>\n",
       "      <td>2.457736e-01</td>\n",
       "      <td>2.458136e-01</td>\n",
       "      <td>-2.521185e-01</td>\n",
       "      <td>-1.776162e-01</td>\n",
       "      <td>-2.588227e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.338638e-01</td>\n",
       "      <td>5.340192e-01</td>\n",
       "      <td>5.338080e-01</td>\n",
       "      <td>5.339054e-01</td>\n",
       "      <td>-6.396828e-02</td>\n",
       "      <td>-1.345711e-01</td>\n",
       "      <td>-1.350566e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.302785e+00</td>\n",
       "      <td>2.302313e+00</td>\n",
       "      <td>2.302864e+00</td>\n",
       "      <td>2.302737e+00</td>\n",
       "      <td>1.626979e+02</td>\n",
       "      <td>9.636451e+01</td>\n",
       "      <td>8.272274e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open          High           Low         Close  Volume_(BTC)  \\\n",
       "count  3.161057e+06  3.161057e+06  3.161057e+06  3.161057e+06  3.161057e+06   \n",
       "mean   3.015734e-13  4.370263e-13  3.686399e-13  6.005367e-13 -5.074202e-14   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -2.150525e+00 -2.150622e+00 -2.634462e+00 -2.634494e+00 -3.099179e-01   \n",
       "25%   -3.932670e-01 -3.934676e-01 -3.933023e-01 -3.932387e-01 -2.974788e-01   \n",
       "50%    2.458171e-01  2.458036e-01  2.457736e-01  2.458136e-01 -2.521185e-01   \n",
       "75%    5.338638e-01  5.340192e-01  5.338080e-01  5.339054e-01 -6.396828e-02   \n",
       "max    2.302785e+00  2.302313e+00  2.302864e+00  2.302737e+00  1.626979e+02   \n",
       "\n",
       "       Volume_(Currency)  Weighted_Price  \n",
       "count       3.161057e+06    3.161057e+06  \n",
       "mean        8.689802e-16   -3.917382e-14  \n",
       "std         1.000000e+00    1.000000e+00  \n",
       "min        -1.844678e-01   -4.246458e-01  \n",
       "25%        -1.832231e-01   -3.772309e-01  \n",
       "50%        -1.776162e-01   -2.588227e-01  \n",
       "75%        -1.345711e-01   -1.350566e-01  \n",
       "max         9.636451e+01    8.272274e+00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_MA_n_days_age(num_days):\n",
    "    num_days_str = str(num_days)\n",
    "    \n",
    "    df[['Open_W_MA_'+num_days_str,'High_W_MA_'+num_days_str,'Low_W_MA_'+num_days_str,'Close_W_MA_'+num_days_str]] = df[['Open','High','Low','Close']].rolling(window=day_window * num_days).mean()\n",
    "    df[['Open_MA_'+num_days_str,'High_MA_'+num_days_str,'Low_MA_'+num_days_str,'Close_MA_'+num_days_str]] = df[['Open','High','Low','Close']].rolling(window=day_window * num_days).mean()\n",
    "    df[['Open_EMA_'+num_days_str,'High_EMA_'+num_days_str,'Low_EMA_'+num_days_str,'Close_EMA_'+num_days_str]] = df[['Open','High','Low','Close']].ewm(span=day_window * num_days).mean()\n",
    "\n",
    "    df[['Open_MAX_'+num_days_str,'High_MAX_'+num_days_str,'Low_MAX_'+num_days_str,'Close_MAX_'+num_days_str]] = df[['Open','High','Low','Close']].rolling(window=day_window * num_days).max()\n",
    "    df[['Open_MIN_'+num_days_str,'High_MIN_'+num_days_str,'Low_MIN_'+num_days_str,'Close_MIN_'+num_days_str]] = df[['Open','High','Low','Close']].rolling(window=day_window * num_days).min()\n",
    "\n",
    "    df['Open_TENKAN_'+num_days_str] = 0.5 * (df['Open_MIN_'+num_days_str] + df['Open_MAX_'+num_days_str])\n",
    "    df['High_TENKAN_'+num_days_str] = 0.5 * (df['High_MIN_'+num_days_str] + df['High_MAX_'+num_days_str])\n",
    "    df['Low_TENKAN_'+num_days_str] = 0.5 * (df['Low_MIN_'+num_days_str] + df['Low_MAX_'+num_days_str])\n",
    "    df['Close_TENKAN_'+num_days_str] = 0.5 * (df['Close_MIN_'+num_days_str] + df['Close_MAX_'+num_days_str])\n",
    "\n",
    "def add_prices_n_days_ago(num_days):\n",
    "    num_days_str = str(num_days)\n",
    "    df[['Open_S_'+num_days_str,'High_S_'+num_days_str,'Low_S_'+num_days_str,'Close_S_'+num_days_str]] = df[['Open','High','Low','Close']].shift(day_window * num_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day_window = 60 * 24\n",
    "\n",
    "df[['Open_W','High_W','Low_W','Close_W']] = df[['Open','High','Low','Close']].divide(df['Volume_(BTC)'],axis = 0)\n",
    "\n",
    "add_prices_n_days_ago(1)\n",
    "add_prices_n_days_ago(2)\n",
    "add_prices_n_days_ago(3)\n",
    "\n",
    "add_MA_n_days_age(5)\n",
    "add_MA_n_days_age(10)\n",
    "add_MA_n_days_age(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plusieurs y gains selon différents temps + vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean = (df['Open'] + df['Close']) * 0.5\n",
    "res = mean.shift(day_window)\n",
    "\n",
    "df['y'] = ((mean - res) > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume_(BTC)</th>\n",
       "      <th>Volume_(Currency)</th>\n",
       "      <th>Weighted_Price</th>\n",
       "      <th>Open_W</th>\n",
       "      <th>High_W</th>\n",
       "      <th>Low_W</th>\n",
       "      <th>...</th>\n",
       "      <th>Close_MAX_15</th>\n",
       "      <th>Open_MIN_15</th>\n",
       "      <th>High_MIN_15</th>\n",
       "      <th>Low_MIN_15</th>\n",
       "      <th>Close_MIN_15</th>\n",
       "      <th>Open_TENKAN_15</th>\n",
       "      <th>High_TENKAN_15</th>\n",
       "      <th>Low_TENKAN_15</th>\n",
       "      <th>Close_TENKAN_15</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28593</th>\n",
       "      <td>-1.893913</td>\n",
       "      <td>-1.894032</td>\n",
       "      <td>-1.893733</td>\n",
       "      <td>-1.893842</td>\n",
       "      <td>0.501800</td>\n",
       "      <td>-0.181067</td>\n",
       "      <td>-0.423575</td>\n",
       "      <td>-3.774235</td>\n",
       "      <td>-3.774472</td>\n",
       "      <td>-3.773878</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.804793</td>\n",
       "      <td>-1.956087</td>\n",
       "      <td>-1.956200</td>\n",
       "      <td>-1.955912</td>\n",
       "      <td>-1.956014</td>\n",
       "      <td>-1.880474</td>\n",
       "      <td>-1.880594</td>\n",
       "      <td>-1.880294</td>\n",
       "      <td>-1.880403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2278155</th>\n",
       "      <td>0.332510</td>\n",
       "      <td>0.332200</td>\n",
       "      <td>0.331751</td>\n",
       "      <td>0.332296</td>\n",
       "      <td>-0.247487</td>\n",
       "      <td>-0.165670</td>\n",
       "      <td>-0.228561</td>\n",
       "      <td>-1.343544</td>\n",
       "      <td>-1.342293</td>\n",
       "      <td>-1.340477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358198</td>\n",
       "      <td>0.313370</td>\n",
       "      <td>0.313123</td>\n",
       "      <td>0.313657</td>\n",
       "      <td>0.313378</td>\n",
       "      <td>0.335775</td>\n",
       "      <td>0.335584</td>\n",
       "      <td>0.336005</td>\n",
       "      <td>0.335788</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855185</th>\n",
       "      <td>0.104927</td>\n",
       "      <td>0.104799</td>\n",
       "      <td>0.105258</td>\n",
       "      <td>0.105103</td>\n",
       "      <td>-0.257139</td>\n",
       "      <td>-0.174195</td>\n",
       "      <td>-0.298483</td>\n",
       "      <td>-0.408054</td>\n",
       "      <td>-0.407557</td>\n",
       "      <td>-0.409342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153242</td>\n",
       "      <td>0.034635</td>\n",
       "      <td>0.034371</td>\n",
       "      <td>0.034898</td>\n",
       "      <td>0.034630</td>\n",
       "      <td>0.093990</td>\n",
       "      <td>0.094473</td>\n",
       "      <td>0.093919</td>\n",
       "      <td>0.093936</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975332</th>\n",
       "      <td>0.018460</td>\n",
       "      <td>0.019898</td>\n",
       "      <td>0.018209</td>\n",
       "      <td>0.018455</td>\n",
       "      <td>6.413254</td>\n",
       "      <td>0.924690</td>\n",
       "      <td>-0.317968</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.003103</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025830</td>\n",
       "      <td>-0.017528</td>\n",
       "      <td>-0.017808</td>\n",
       "      <td>-0.018051</td>\n",
       "      <td>-0.017898</td>\n",
       "      <td>0.004048</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.003939</td>\n",
       "      <td>0.003966</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799366</th>\n",
       "      <td>0.929671</td>\n",
       "      <td>0.929314</td>\n",
       "      <td>0.930065</td>\n",
       "      <td>0.929666</td>\n",
       "      <td>-0.301252</td>\n",
       "      <td>-0.176250</td>\n",
       "      <td>0.196505</td>\n",
       "      <td>-3.086028</td>\n",
       "      <td>-3.084843</td>\n",
       "      <td>-3.087335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.935945</td>\n",
       "      <td>0.829257</td>\n",
       "      <td>0.828905</td>\n",
       "      <td>0.829643</td>\n",
       "      <td>0.829251</td>\n",
       "      <td>0.882233</td>\n",
       "      <td>0.882249</td>\n",
       "      <td>0.882266</td>\n",
       "      <td>0.882598</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681392</th>\n",
       "      <td>-0.549072</td>\n",
       "      <td>-0.544833</td>\n",
       "      <td>-0.550629</td>\n",
       "      <td>-0.544942</td>\n",
       "      <td>0.923724</td>\n",
       "      <td>-0.116075</td>\n",
       "      <td>-0.389913</td>\n",
       "      <td>-0.594411</td>\n",
       "      <td>-0.589822</td>\n",
       "      <td>-0.596097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048714</td>\n",
       "      <td>-0.810614</td>\n",
       "      <td>-0.808634</td>\n",
       "      <td>-0.863136</td>\n",
       "      <td>-0.808486</td>\n",
       "      <td>-0.380958</td>\n",
       "      <td>-0.380111</td>\n",
       "      <td>-0.407055</td>\n",
       "      <td>-0.379886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075807</th>\n",
       "      <td>0.343181</td>\n",
       "      <td>0.342871</td>\n",
       "      <td>0.343531</td>\n",
       "      <td>0.343189</td>\n",
       "      <td>-0.285591</td>\n",
       "      <td>-0.176988</td>\n",
       "      <td>-0.224373</td>\n",
       "      <td>-1.201655</td>\n",
       "      <td>-1.200568</td>\n",
       "      <td>-1.202877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355732</td>\n",
       "      <td>0.202790</td>\n",
       "      <td>0.203550</td>\n",
       "      <td>0.203128</td>\n",
       "      <td>0.202906</td>\n",
       "      <td>0.279285</td>\n",
       "      <td>0.279565</td>\n",
       "      <td>0.279602</td>\n",
       "      <td>0.279319</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511248</th>\n",
       "      <td>0.272267</td>\n",
       "      <td>0.272015</td>\n",
       "      <td>0.272558</td>\n",
       "      <td>0.272329</td>\n",
       "      <td>-0.255896</td>\n",
       "      <td>-0.169973</td>\n",
       "      <td>-0.250092</td>\n",
       "      <td>-1.063974</td>\n",
       "      <td>-1.062989</td>\n",
       "      <td>-1.065113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340215</td>\n",
       "      <td>0.155737</td>\n",
       "      <td>0.156243</td>\n",
       "      <td>0.152939</td>\n",
       "      <td>0.154571</td>\n",
       "      <td>0.247972</td>\n",
       "      <td>0.248070</td>\n",
       "      <td>0.246748</td>\n",
       "      <td>0.247393</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248788</th>\n",
       "      <td>0.333894</td>\n",
       "      <td>0.333584</td>\n",
       "      <td>0.334242</td>\n",
       "      <td>0.333901</td>\n",
       "      <td>-0.260675</td>\n",
       "      <td>-0.169594</td>\n",
       "      <td>-0.227943</td>\n",
       "      <td>-1.280882</td>\n",
       "      <td>-1.279694</td>\n",
       "      <td>-1.282219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353019</td>\n",
       "      <td>0.301013</td>\n",
       "      <td>0.300928</td>\n",
       "      <td>0.300443</td>\n",
       "      <td>0.300105</td>\n",
       "      <td>0.326889</td>\n",
       "      <td>0.327184</td>\n",
       "      <td>0.326723</td>\n",
       "      <td>0.326562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644555</th>\n",
       "      <td>-0.633232</td>\n",
       "      <td>-0.633459</td>\n",
       "      <td>-0.633032</td>\n",
       "      <td>-0.633271</td>\n",
       "      <td>0.159043</td>\n",
       "      <td>-0.162358</td>\n",
       "      <td>-0.395360</td>\n",
       "      <td>-3.981525</td>\n",
       "      <td>-3.982952</td>\n",
       "      <td>-3.980264</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.584249</td>\n",
       "      <td>-0.974272</td>\n",
       "      <td>-0.974470</td>\n",
       "      <td>-0.979348</td>\n",
       "      <td>-0.979552</td>\n",
       "      <td>-0.779277</td>\n",
       "      <td>-0.779492</td>\n",
       "      <td>-0.781676</td>\n",
       "      <td>-0.781900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open      High       Low     Close  Volume_(BTC)  \\\n",
       "28593   -1.893913 -1.894032 -1.893733 -1.893842      0.501800   \n",
       "2278155  0.332510  0.332200  0.331751  0.332296     -0.247487   \n",
       "1855185  0.104927  0.104799  0.105258  0.105103     -0.257139   \n",
       "1975332  0.018460  0.019898  0.018209  0.018455      6.413254   \n",
       "2799366  0.929671  0.929314  0.930065  0.929666     -0.301252   \n",
       "681392  -0.549072 -0.544833 -0.550629 -0.544942      0.923724   \n",
       "2075807  0.343181  0.342871  0.343531  0.343189     -0.285591   \n",
       "1511248  0.272267  0.272015  0.272558  0.272329     -0.255896   \n",
       "1248788  0.333894  0.333584  0.334242  0.333901     -0.260675   \n",
       "644555  -0.633232 -0.633459 -0.633032 -0.633271      0.159043   \n",
       "\n",
       "         Volume_(Currency)  Weighted_Price    Open_W    High_W     Low_W ...  \\\n",
       "28593            -0.181067       -0.423575 -3.774235 -3.774472 -3.773878 ...   \n",
       "2278155          -0.165670       -0.228561 -1.343544 -1.342293 -1.340477 ...   \n",
       "1855185          -0.174195       -0.298483 -0.408054 -0.407557 -0.409342 ...   \n",
       "1975332           0.924690       -0.317968  0.002878  0.003103  0.002839 ...   \n",
       "2799366          -0.176250        0.196505 -3.086028 -3.084843 -3.087335 ...   \n",
       "681392           -0.116075       -0.389913 -0.594411 -0.589822 -0.596097 ...   \n",
       "2075807          -0.176988       -0.224373 -1.201655 -1.200568 -1.202877 ...   \n",
       "1511248          -0.169973       -0.250092 -1.063974 -1.062989 -1.065113 ...   \n",
       "1248788          -0.169594       -0.227943 -1.280882 -1.279694 -1.282219 ...   \n",
       "644555           -0.162358       -0.395360 -3.981525 -3.982952 -3.980264 ...   \n",
       "\n",
       "         Close_MAX_15  Open_MIN_15  High_MIN_15  Low_MIN_15  Close_MIN_15  \\\n",
       "28593       -1.804793    -1.956087    -1.956200   -1.955912     -1.956014   \n",
       "2278155      0.358198     0.313370     0.313123    0.313657      0.313378   \n",
       "1855185      0.153242     0.034635     0.034371    0.034898      0.034630   \n",
       "1975332      0.025830    -0.017528    -0.017808   -0.018051     -0.017898   \n",
       "2799366      0.935945     0.829257     0.828905    0.829643      0.829251   \n",
       "681392       0.048714    -0.810614    -0.808634   -0.863136     -0.808486   \n",
       "2075807      0.355732     0.202790     0.203550    0.203128      0.202906   \n",
       "1511248      0.340215     0.155737     0.156243    0.152939      0.154571   \n",
       "1248788      0.353019     0.301013     0.300928    0.300443      0.300105   \n",
       "644555      -0.584249    -0.974272    -0.974470   -0.979348     -0.979552   \n",
       "\n",
       "         Open_TENKAN_15  High_TENKAN_15  Low_TENKAN_15  Close_TENKAN_15  y  \n",
       "28593         -1.880474       -1.880594      -1.880294        -1.880403  0  \n",
       "2278155        0.335775        0.335584       0.336005         0.335788  0  \n",
       "1855185        0.093990        0.094473       0.093919         0.093936  0  \n",
       "1975332        0.004048        0.003861       0.003939         0.003966  1  \n",
       "2799366        0.882233        0.882249       0.882266         0.882598  1  \n",
       "681392        -0.380958       -0.380111      -0.407055        -0.379886  1  \n",
       "2075807        0.279285        0.279565       0.279602         0.279319  1  \n",
       "1511248        0.247972        0.248070       0.246748         0.247393  0  \n",
       "1248788        0.326889        0.327184       0.326723         0.326562  0  \n",
       "644555        -0.779277       -0.779492      -0.781676        -0.781900  0  \n",
       "\n",
       "[10 rows x 96 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume_(BTC)</th>\n",
       "      <th>Volume_(Currency)</th>\n",
       "      <th>Weighted_Price</th>\n",
       "      <th>Open_W</th>\n",
       "      <th>High_W</th>\n",
       "      <th>Low_W</th>\n",
       "      <th>...</th>\n",
       "      <th>Close_MAX_15</th>\n",
       "      <th>Open_MIN_15</th>\n",
       "      <th>High_MIN_15</th>\n",
       "      <th>Low_MIN_15</th>\n",
       "      <th>Close_MIN_15</th>\n",
       "      <th>Open_TENKAN_15</th>\n",
       "      <th>High_TENKAN_15</th>\n",
       "      <th>Low_TENKAN_15</th>\n",
       "      <th>Close_TENKAN_15</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "      <td>3.139458e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.392586e-13</td>\n",
       "      <td>-5.906626e-13</td>\n",
       "      <td>5.790729e-14</td>\n",
       "      <td>-4.446534e-13</td>\n",
       "      <td>6.607257e-14</td>\n",
       "      <td>1.272720e-14</td>\n",
       "      <td>-7.354502e-14</td>\n",
       "      <td>-2.902058e-17</td>\n",
       "      <td>-1.760952e-16</td>\n",
       "      <td>1.528909e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.545786e-13</td>\n",
       "      <td>-2.427295e-12</td>\n",
       "      <td>7.563479e-14</td>\n",
       "      <td>4.981674e-13</td>\n",
       "      <td>-7.594128e-13</td>\n",
       "      <td>1.256374e-12</td>\n",
       "      <td>-4.448784e-12</td>\n",
       "      <td>-3.648817e-12</td>\n",
       "      <td>-1.063144e-12</td>\n",
       "      <td>5.553561e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.969263e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.183942e+00</td>\n",
       "      <td>-2.184021e+00</td>\n",
       "      <td>-2.672430e+00</td>\n",
       "      <td>-2.672439e+00</td>\n",
       "      <td>-3.100156e-01</td>\n",
       "      <td>-1.851188e-01</td>\n",
       "      <td>-4.263604e-01</td>\n",
       "      <td>-3.251624e+02</td>\n",
       "      <td>-3.250301e+02</td>\n",
       "      <td>-3.252853e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.080054e+00</td>\n",
       "      <td>-2.102651e+00</td>\n",
       "      <td>-2.103383e+00</td>\n",
       "      <td>-2.514825e+00</td>\n",
       "      <td>-2.516158e+00</td>\n",
       "      <td>-2.063340e+00</td>\n",
       "      <td>-2.064458e+00</td>\n",
       "      <td>-2.045646e+00</td>\n",
       "      <td>-2.046634e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-3.881069e-01</td>\n",
       "      <td>-3.878531e-01</td>\n",
       "      <td>-3.884049e-01</td>\n",
       "      <td>-3.882137e-01</td>\n",
       "      <td>-2.977100e-01</td>\n",
       "      <td>-1.838289e-01</td>\n",
       "      <td>-3.769825e-01</td>\n",
       "      <td>-1.635690e-03</td>\n",
       "      <td>-1.634168e-03</td>\n",
       "      <td>-1.635967e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.817015e-01</td>\n",
       "      <td>-4.439232e-01</td>\n",
       "      <td>-4.454905e-01</td>\n",
       "      <td>-4.261698e-01</td>\n",
       "      <td>-4.277904e-01</td>\n",
       "      <td>-4.057692e-01</td>\n",
       "      <td>-4.057031e-01</td>\n",
       "      <td>-4.081473e-01</td>\n",
       "      <td>-4.090605e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.377383e-01</td>\n",
       "      <td>2.377454e-01</td>\n",
       "      <td>2.376952e-01</td>\n",
       "      <td>2.377903e-01</td>\n",
       "      <td>-2.522784e-01</td>\n",
       "      <td>-1.781013e-01</td>\n",
       "      <td>-2.600888e-01</td>\n",
       "      <td>1.490599e-04</td>\n",
       "      <td>1.501120e-04</td>\n",
       "      <td>1.486858e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>2.438378e-01</td>\n",
       "      <td>2.741989e-01</td>\n",
       "      <td>2.751972e-01</td>\n",
       "      <td>2.802574e-01</td>\n",
       "      <td>2.797808e-01</td>\n",
       "      <td>2.623724e-01</td>\n",
       "      <td>2.623112e-01</td>\n",
       "      <td>2.668872e-01</td>\n",
       "      <td>2.659238e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.276694e-01</td>\n",
       "      <td>5.277287e-01</td>\n",
       "      <td>5.276388e-01</td>\n",
       "      <td>5.276713e-01</td>\n",
       "      <td>-6.478471e-02</td>\n",
       "      <td>-1.345290e-01</td>\n",
       "      <td>-1.364458e-01</td>\n",
       "      <td>2.967752e-03</td>\n",
       "      <td>2.967989e-03</td>\n",
       "      <td>2.967562e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>5.382577e-01</td>\n",
       "      <td>5.675958e-01</td>\n",
       "      <td>5.660385e-01</td>\n",
       "      <td>5.780274e-01</td>\n",
       "      <td>5.762685e-01</td>\n",
       "      <td>5.394798e-01</td>\n",
       "      <td>5.385775e-01</td>\n",
       "      <td>5.390893e-01</td>\n",
       "      <td>5.383868e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.311080e+00</td>\n",
       "      <td>2.310591e+00</td>\n",
       "      <td>2.311170e+00</td>\n",
       "      <td>2.311028e+00</td>\n",
       "      <td>1.621828e+02</td>\n",
       "      <td>9.604464e+01</td>\n",
       "      <td>8.246152e+00</td>\n",
       "      <td>1.272633e+03</td>\n",
       "      <td>1.272879e+03</td>\n",
       "      <td>1.272564e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>2.253494e+00</td>\n",
       "      <td>2.193020e+00</td>\n",
       "      <td>2.191008e+00</td>\n",
       "      <td>2.164114e+00</td>\n",
       "      <td>2.165990e+00</td>\n",
       "      <td>2.227555e+00</td>\n",
       "      <td>2.226237e+00</td>\n",
       "      <td>2.231059e+00</td>\n",
       "      <td>2.231849e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open          High           Low         Close  Volume_(BTC)  \\\n",
       "count  3.139458e+06  3.139458e+06  3.139458e+06  3.139458e+06  3.139458e+06   \n",
       "mean  -3.392586e-13 -5.906626e-13  5.790729e-14 -4.446534e-13  6.607257e-14   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -2.183942e+00 -2.184021e+00 -2.672430e+00 -2.672439e+00 -3.100156e-01   \n",
       "25%   -3.881069e-01 -3.878531e-01 -3.884049e-01 -3.882137e-01 -2.977100e-01   \n",
       "50%    2.377383e-01  2.377454e-01  2.376952e-01  2.377903e-01 -2.522784e-01   \n",
       "75%    5.276694e-01  5.277287e-01  5.276388e-01  5.276713e-01 -6.478471e-02   \n",
       "max    2.311080e+00  2.310591e+00  2.311170e+00  2.311028e+00  1.621828e+02   \n",
       "\n",
       "       Volume_(Currency)  Weighted_Price        Open_W        High_W  \\\n",
       "count       3.139458e+06    3.139458e+06  3.139458e+06  3.139458e+06   \n",
       "mean        1.272720e-14   -7.354502e-14 -2.902058e-17 -1.760952e-16   \n",
       "std         1.000000e+00    1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min        -1.851188e-01   -4.263604e-01 -3.251624e+02 -3.250301e+02   \n",
       "25%        -1.838289e-01   -3.769825e-01 -1.635690e-03 -1.634168e-03   \n",
       "50%        -1.781013e-01   -2.600888e-01  1.490599e-04  1.501120e-04   \n",
       "75%        -1.345290e-01   -1.364458e-01  2.967752e-03  2.967989e-03   \n",
       "max         9.604464e+01    8.246152e+00  1.272633e+03  1.272879e+03   \n",
       "\n",
       "              Low_W      ...       Close_MAX_15   Open_MIN_15   High_MIN_15  \\\n",
       "count  3.139458e+06      ...       3.139458e+06  3.139458e+06  3.139458e+06   \n",
       "mean   1.528909e-16      ...      -3.545786e-13 -2.427295e-12  7.563479e-14   \n",
       "std    1.000000e+00      ...       1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -3.252853e+02      ...      -2.080054e+00 -2.102651e+00 -2.103383e+00   \n",
       "25%   -1.635967e-03      ...      -3.817015e-01 -4.439232e-01 -4.454905e-01   \n",
       "50%    1.486858e-04      ...       2.438378e-01  2.741989e-01  2.751972e-01   \n",
       "75%    2.967562e-03      ...       5.382577e-01  5.675958e-01  5.660385e-01   \n",
       "max    1.272564e+03      ...       2.253494e+00  2.193020e+00  2.191008e+00   \n",
       "\n",
       "         Low_MIN_15  Close_MIN_15  Open_TENKAN_15  High_TENKAN_15  \\\n",
       "count  3.139458e+06  3.139458e+06    3.139458e+06    3.139458e+06   \n",
       "mean   4.981674e-13 -7.594128e-13    1.256374e-12   -4.448784e-12   \n",
       "std    1.000000e+00  1.000000e+00    1.000000e+00    1.000000e+00   \n",
       "min   -2.514825e+00 -2.516158e+00   -2.063340e+00   -2.064458e+00   \n",
       "25%   -4.261698e-01 -4.277904e-01   -4.057692e-01   -4.057031e-01   \n",
       "50%    2.802574e-01  2.797808e-01    2.623724e-01    2.623112e-01   \n",
       "75%    5.780274e-01  5.762685e-01    5.394798e-01    5.385775e-01   \n",
       "max    2.164114e+00  2.165990e+00    2.227555e+00    2.226237e+00   \n",
       "\n",
       "       Low_TENKAN_15  Close_TENKAN_15             y  \n",
       "count   3.139458e+06     3.139458e+06  3.139458e+06  \n",
       "mean   -3.648817e-12    -1.063144e-12  5.553561e-01  \n",
       "std     1.000000e+00     1.000000e+00  4.969263e-01  \n",
       "min    -2.045646e+00    -2.046634e+00  0.000000e+00  \n",
       "25%    -4.081473e-01    -4.090605e-01  0.000000e+00  \n",
       "50%     2.668872e-01     2.659238e-01  1.000000e+00  \n",
       "75%     5.390893e-01     5.383868e-01  1.000000e+00  \n",
       "max     2.231059e+00     2.231849e+00  1.000000e+00  \n",
       "\n",
       "[8 rows x 96 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.dropna(how ='any')\n",
    "\n",
    "y = df.y\n",
    "df = (df-df.mean())/df.std()\n",
    "df.y = y\n",
    "df = df.reset_index()\n",
    "df = df.drop('index',1)\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "length_test2=int(0.8*len(df.index))\n",
    "window = day_window*30 * 2\n",
    "\n",
    "index = df.index\n",
    "index= index[0:length_test2]\n",
    "\n",
    "indexList = [index[i:min(i + window,len(index))] for i in range(0, len(index), window)]\n",
    "trainIndex = []\n",
    "testIndex = []\n",
    "\n",
    "for i in range(len(indexList)):\n",
    "    if i%9 != 0:\n",
    "        trainIndex += indexList[i].tolist()\n",
    "    else:\n",
    "        testIndex  += indexList[i].tolist()\n",
    "        \n",
    "print(len(trainIndex+testIndex) == len(index))\n",
    "print(len(indexList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df.iloc[trainIndex,:]\n",
    "test1=df.iloc[testIndex,:]\n",
    "test2=df.drop(train.index).drop(test1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2165966/2165966 [==============================] - 148s 69us/step - loss: 0.4456 - binary_accuracy: 0.7655\n",
      "Epoch 2/200\n",
      "2165966/2165966 [==============================] - 156s 72us/step - loss: 0.3509 - binary_accuracy: 0.8304\n",
      "Epoch 3/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.3152 - binary_accuracy: 0.8509\n",
      "Epoch 4/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.2968 - binary_accuracy: 0.8613\n",
      "Epoch 5/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.2819 - binary_accuracy: 0.8697\n",
      "Epoch 6/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.2745 - binary_accuracy: 0.8752\n",
      "Epoch 7/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.2648 - binary_accuracy: 0.8809\n",
      "Epoch 8/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.2672 - binary_accuracy: 0.8798\n",
      "Epoch 9/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.2649 - binary_accuracy: 0.8822\n",
      "Epoch 10/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.2613 - binary_accuracy: 0.8856\n",
      "Epoch 11/200\n",
      "2165966/2165966 [==============================] - 146s 67us/step - loss: 0.2662 - binary_accuracy: 0.8844\n",
      "Epoch 12/200\n",
      "2165966/2165966 [==============================] - 146s 67us/step - loss: 0.2584 - binary_accuracy: 0.8893\n",
      "Epoch 13/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.2636 - binary_accuracy: 0.8878\n",
      "Epoch 14/200\n",
      "2165966/2165966 [==============================] - 149s 69us/step - loss: 0.2493 - binary_accuracy: 0.8940\n",
      "Epoch 15/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.2493 - binary_accuracy: 0.8942\n",
      "Epoch 16/200\n",
      "2165966/2165966 [==============================] - 157s 72us/step - loss: 0.2456 - binary_accuracy: 0.8964\n",
      "Epoch 17/200\n",
      "2165966/2165966 [==============================] - 152s 70us/step - loss: 0.2522 - binary_accuracy: 0.8946\n",
      "Epoch 18/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.2441 - binary_accuracy: 0.8986\n",
      "Epoch 19/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.2516 - binary_accuracy: 0.8956\n",
      "Epoch 20/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.2519 - binary_accuracy: 0.8959\n",
      "Epoch 21/200\n",
      "2165966/2165966 [==============================] - 148s 68us/step - loss: 0.2422 - binary_accuracy: 0.8988\n",
      "Epoch 22/200\n",
      "2165966/2165966 [==============================] - 148s 68us/step - loss: 0.2450 - binary_accuracy: 0.8974\n",
      "Epoch 23/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.2460 - binary_accuracy: 0.8978\n",
      "Epoch 24/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.2384 - binary_accuracy: 0.9011\n",
      "Epoch 25/200\n",
      "2165966/2165966 [==============================] - 148s 68us/step - loss: 0.2374 - binary_accuracy: 0.9029\n",
      "Epoch 26/200\n",
      "2165966/2165966 [==============================] - 148s 69us/step - loss: 0.2486 - binary_accuracy: 0.9021\n",
      "Epoch 27/200\n",
      "2165966/2165966 [==============================] - 149s 69us/step - loss: 0.2465 - binary_accuracy: 0.9020\n",
      "Epoch 28/200\n",
      "2165966/2165966 [==============================] - 150s 69us/step - loss: 0.2625 - binary_accuracy: 0.8957\n",
      "Epoch 29/200\n",
      "2165966/2165966 [==============================] - 149s 69us/step - loss: 0.2622 - binary_accuracy: 0.8954\n",
      "Epoch 30/200\n",
      "2165966/2165966 [==============================] - 149s 69us/step - loss: 0.2595 - binary_accuracy: 0.8965\n",
      "Epoch 31/200\n",
      "2165966/2165966 [==============================] - 148s 68us/step - loss: 0.2527 - binary_accuracy: 0.8983\n",
      "Epoch 32/200\n",
      "2165966/2165966 [==============================] - 148s 68us/step - loss: 0.2689 - binary_accuracy: 0.8940\n",
      "Epoch 33/200\n",
      "2165966/2165966 [==============================] - 149s 69us/step - loss: 0.2517 - binary_accuracy: 0.9027\n",
      "Epoch 34/200\n",
      "2165966/2165966 [==============================] - 149s 69us/step - loss: 0.2626 - binary_accuracy: 0.8969\n",
      "Epoch 35/200\n",
      "2165966/2165966 [==============================] - 149s 69us/step - loss: 0.2652 - binary_accuracy: 0.8948\n",
      "Epoch 36/200\n",
      "2165966/2165966 [==============================] - 149s 69us/step - loss: 0.2561 - binary_accuracy: 0.9012\n",
      "Epoch 37/200\n",
      "2165966/2165966 [==============================] - 150s 69us/step - loss: 0.2521 - binary_accuracy: 0.9029\n",
      "Epoch 38/200\n",
      "2165966/2165966 [==============================] - 177s 82us/step - loss: 0.2497 - binary_accuracy: 0.9048\n",
      "Epoch 39/200\n",
      "2165966/2165966 [==============================] - 150s 69us/step - loss: 0.2466 - binary_accuracy: 0.9059\n",
      "Epoch 40/200\n",
      "2165966/2165966 [==============================] - 152s 70us/step - loss: 0.2413 - binary_accuracy: 0.9085\n",
      "Epoch 41/200\n",
      "2165966/2165966 [==============================] - 151s 70us/step - loss: 0.2444 - binary_accuracy: 0.9077\n",
      "Epoch 42/200\n",
      "2165966/2165966 [==============================] - 150s 69us/step - loss: 0.2517 - binary_accuracy: 0.9036\n",
      "Epoch 43/200\n",
      "2165966/2165966 [==============================] - 150s 69us/step - loss: 0.2497 - binary_accuracy: 0.9033\n",
      "Epoch 44/200\n",
      "2165966/2165966 [==============================] - 150s 69us/step - loss: 0.2502 - binary_accuracy: 0.9023\n",
      "Epoch 45/200\n",
      "2165966/2165966 [==============================] - 152s 70us/step - loss: 0.2416 - binary_accuracy: 0.9066\n",
      "Epoch 46/200\n",
      "2165966/2165966 [==============================] - 153s 71us/step - loss: 0.2470 - binary_accuracy: 0.9019\n",
      "Epoch 47/200\n",
      "2165966/2165966 [==============================] - 153s 70us/step - loss: 0.2357 - binary_accuracy: 0.9081\n",
      "Epoch 48/200\n",
      "2165966/2165966 [==============================] - 153s 70us/step - loss: 0.2271 - binary_accuracy: 0.9100\n",
      "Epoch 49/200\n",
      "2165966/2165966 [==============================] - 153s 71us/step - loss: 0.2234 - binary_accuracy: 0.9138\n",
      "Epoch 50/200\n",
      "2165966/2165966 [==============================] - 153s 71us/step - loss: 0.2163 - binary_accuracy: 0.9170\n",
      "Epoch 51/200\n",
      "2165966/2165966 [==============================] - 172s 79us/step - loss: 0.2121 - binary_accuracy: 0.9191\n",
      "Epoch 52/200\n",
      "2165966/2165966 [==============================] - 173s 80us/step - loss: 0.2161 - binary_accuracy: 0.9178\n",
      "Epoch 53/200\n",
      "2165966/2165966 [==============================] - ETA: 0s - loss: 0.2179 - binary_accuracy: 0.917 - 153s 71us/step - loss: 0.2179 - binary_accuracy: 0.9174\n",
      "Epoch 54/200\n",
      "2165966/2165966 [==============================] - 153s 71us/step - loss: 0.2243 - binary_accuracy: 0.9159\n",
      "Epoch 55/200\n",
      "2165966/2165966 [==============================] - 151s 70us/step - loss: 0.2324 - binary_accuracy: 0.9119\n",
      "Epoch 56/200\n",
      "2165966/2165966 [==============================] - 151s 70us/step - loss: 0.2453 - binary_accuracy: 0.9057\n",
      "Epoch 57/200\n",
      "2165966/2165966 [==============================] - 152s 70us/step - loss: 0.2305 - binary_accuracy: 0.9107\n",
      "Epoch 58/200\n",
      "2165966/2165966 [==============================] - 151s 70us/step - loss: 0.2136 - binary_accuracy: 0.9170\n",
      "Epoch 59/200\n",
      "2165966/2165966 [==============================] - 152s 70us/step - loss: 0.2218 - binary_accuracy: 0.9164\n",
      "Epoch 60/200\n",
      "2165966/2165966 [==============================] - 152s 70us/step - loss: 0.2223 - binary_accuracy: 0.9167\n",
      "Epoch 61/200\n",
      "2165966/2165966 [==============================] - 153s 71us/step - loss: 0.2275 - binary_accuracy: 0.9146\n",
      "Epoch 62/200\n",
      "2165966/2165966 [==============================] - 155s 71us/step - loss: 0.2387 - binary_accuracy: 0.9071\n",
      "Epoch 63/200\n",
      "2165966/2165966 [==============================] - 157s 72us/step - loss: 0.2258 - binary_accuracy: 0.9135\n",
      "Epoch 64/200\n",
      "2165966/2165966 [==============================] - 155s 72us/step - loss: 0.2190 - binary_accuracy: 0.9166\n",
      "Epoch 65/200\n",
      "2165966/2165966 [==============================] - 154s 71us/step - loss: 0.2190 - binary_accuracy: 0.9171\n",
      "Epoch 66/200\n",
      "2165966/2165966 [==============================] - 154s 71us/step - loss: 0.2191 - binary_accuracy: 0.9178\n",
      "Epoch 67/200\n",
      "2165966/2165966 [==============================] - 153s 71us/step - loss: 0.2148 - binary_accuracy: 0.9190\n",
      "Epoch 68/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2165966/2165966 [==============================] - 139s 64us/step - loss: 0.2128 - binary_accuracy: 0.9189\n",
      "Epoch 69/200\n",
      "2165966/2165966 [==============================] - 140s 64us/step - loss: 0.2141 - binary_accuracy: 0.9191\n",
      "Epoch 70/200\n",
      "2165966/2165966 [==============================] - 139s 64us/step - loss: 0.2161 - binary_accuracy: 0.9185\n",
      "Epoch 71/200\n",
      "2165966/2165966 [==============================] - 140s 64us/step - loss: 0.2159 - binary_accuracy: 0.9187\n",
      "Epoch 72/200\n",
      "2165966/2165966 [==============================] - 140s 65us/step - loss: 0.2137 - binary_accuracy: 0.9203\n",
      "Epoch 73/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.2151 - binary_accuracy: 0.9208\n",
      "Epoch 74/200\n",
      "2165966/2165966 [==============================] - 140s 64us/step - loss: 0.2146 - binary_accuracy: 0.9201\n",
      "Epoch 75/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.2134 - binary_accuracy: 0.9213\n",
      "Epoch 76/200\n",
      "2165966/2165966 [==============================] - 142s 66us/step - loss: 0.2145 - binary_accuracy: 0.9199\n",
      "Epoch 77/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.2046 - binary_accuracy: 0.9241\n",
      "Epoch 78/200\n",
      "2165966/2165966 [==============================] - 140s 65us/step - loss: 0.2096 - binary_accuracy: 0.9230\n",
      "Epoch 79/200\n",
      "2165966/2165966 [==============================] - 140s 65us/step - loss: 0.2111 - binary_accuracy: 0.9222\n",
      "Epoch 80/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.2300 - binary_accuracy: 0.9110\n",
      "Epoch 81/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.2294 - binary_accuracy: 0.9075\n",
      "Epoch 82/200\n",
      "2165966/2165966 [==============================] - 142s 65us/step - loss: 0.2201 - binary_accuracy: 0.9152\n",
      "Epoch 83/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.2056 - binary_accuracy: 0.9203\n",
      "Epoch 84/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.2037 - binary_accuracy: 0.9222\n",
      "Epoch 85/200\n",
      "2165966/2165966 [==============================] - 140s 65us/step - loss: 0.2083 - binary_accuracy: 0.9212\n",
      "Epoch 86/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.2051 - binary_accuracy: 0.9232\n",
      "Epoch 87/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.2023 - binary_accuracy: 0.9244\n",
      "Epoch 88/200\n",
      "2165966/2165966 [==============================] - 142s 66us/step - loss: 0.1996 - binary_accuracy: 0.9265\n",
      "Epoch 89/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.1973 - binary_accuracy: 0.9279\n",
      "Epoch 90/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.1970 - binary_accuracy: 0.9278\n",
      "Epoch 91/200\n",
      "2165966/2165966 [==============================] - 142s 65us/step - loss: 0.2006 - binary_accuracy: 0.9271\n",
      "Epoch 92/200\n",
      "2165966/2165966 [==============================] - 142s 66us/step - loss: 0.2030 - binary_accuracy: 0.9255\n",
      "Epoch 93/200\n",
      "2165966/2165966 [==============================] - 142s 65us/step - loss: 0.2023 - binary_accuracy: 0.9265\n",
      "Epoch 94/200\n",
      "2165966/2165966 [==============================] - 144s 66us/step - loss: 0.2011 - binary_accuracy: 0.9264\n",
      "Epoch 95/200\n",
      "2165966/2165966 [==============================] - 142s 66us/step - loss: 0.2090 - binary_accuracy: 0.9226\n",
      "Epoch 96/200\n",
      "2165966/2165966 [==============================] - 142s 66us/step - loss: 0.2143 - binary_accuracy: 0.9194\n",
      "Epoch 97/200\n",
      "2165966/2165966 [==============================] - 142s 66us/step - loss: 0.2101 - binary_accuracy: 0.9180\n",
      "Epoch 98/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.2076 - binary_accuracy: 0.9206\n",
      "Epoch 99/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.1998 - binary_accuracy: 0.9275\n",
      "Epoch 100/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.2062 - binary_accuracy: 0.9232\n",
      "Epoch 101/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.2068 - binary_accuracy: 0.9230\n",
      "Epoch 102/200\n",
      "2165966/2165966 [==============================] - 144s 66us/step - loss: 0.2008 - binary_accuracy: 0.9259\n",
      "Epoch 103/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.2033 - binary_accuracy: 0.9254\n",
      "Epoch 104/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.2057 - binary_accuracy: 0.9228\n",
      "Epoch 105/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.1981 - binary_accuracy: 0.9278\n",
      "Epoch 106/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.2081 - binary_accuracy: 0.9227\n",
      "Epoch 107/200\n",
      "2165966/2165966 [==============================] - 144s 66us/step - loss: 0.2067 - binary_accuracy: 0.9226\n",
      "Epoch 108/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.1878 - binary_accuracy: 0.9303\n",
      "Epoch 109/200\n",
      "2165966/2165966 [==============================] - 144s 66us/step - loss: 0.1872 - binary_accuracy: 0.9315\n",
      "Epoch 110/200\n",
      "2165966/2165966 [==============================] - 144s 66us/step - loss: 0.1874 - binary_accuracy: 0.9321\n",
      "Epoch 111/200\n",
      "2165966/2165966 [==============================] - 144s 66us/step - loss: 0.1924 - binary_accuracy: 0.9308\n",
      "Epoch 112/200\n",
      "2165966/2165966 [==============================] - 144s 66us/step - loss: 0.1881 - binary_accuracy: 0.9320\n",
      "Epoch 113/200\n",
      "2165966/2165966 [==============================] - 146s 67us/step - loss: 0.1865 - binary_accuracy: 0.9328\n",
      "Epoch 114/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.1922 - binary_accuracy: 0.9303\n",
      "Epoch 115/200\n",
      "2165966/2165966 [==============================] - 146s 67us/step - loss: 0.1873 - binary_accuracy: 0.9323\n",
      "Epoch 116/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.1984 - binary_accuracy: 0.9278\n",
      "Epoch 117/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.2145 - binary_accuracy: 0.9187\n",
      "Epoch 118/200\n",
      "2165966/2165966 [==============================] - 151s 70us/step - loss: 0.2052 - binary_accuracy: 0.9211\n",
      "Epoch 119/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.2011 - binary_accuracy: 0.9218\n",
      "Epoch 120/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.2098 - binary_accuracy: 0.9200\n",
      "Epoch 121/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.2028 - binary_accuracy: 0.9249\n",
      "Epoch 122/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.2014 - binary_accuracy: 0.9267\n",
      "Epoch 123/200\n",
      "2165966/2165966 [==============================] - 146s 67us/step - loss: 0.1918 - binary_accuracy: 0.9296\n",
      "Epoch 124/200\n",
      "2165966/2165966 [==============================] - 146s 67us/step - loss: 0.1907 - binary_accuracy: 0.9306\n",
      "Epoch 125/200\n",
      "2165966/2165966 [==============================] - 146s 67us/step - loss: 0.1869 - binary_accuracy: 0.9334\n",
      "Epoch 126/200\n",
      "2165966/2165966 [==============================] - 149s 69us/step - loss: 0.1852 - binary_accuracy: 0.9337\n",
      "Epoch 127/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.1830 - binary_accuracy: 0.9342\n",
      "Epoch 128/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.1839 - binary_accuracy: 0.9340\n",
      "Epoch 129/200\n",
      "2165966/2165966 [==============================] - 146s 68us/step - loss: 0.1883 - binary_accuracy: 0.9318\n",
      "Epoch 130/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.1884 - binary_accuracy: 0.9324\n",
      "Epoch 131/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.1892 - binary_accuracy: 0.9313\n",
      "Epoch 132/200\n",
      "2165966/2165966 [==============================] - 148s 68us/step - loss: 0.1913 - binary_accuracy: 0.9291\n",
      "Epoch 133/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.1991 - binary_accuracy: 0.9239\n",
      "Epoch 134/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.1938 - binary_accuracy: 0.9290\n",
      "Epoch 135/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2165966/2165966 [==============================] - 142s 66us/step - loss: 0.1927 - binary_accuracy: 0.9297\n",
      "Epoch 136/200\n",
      "2165966/2165966 [==============================] - 140s 65us/step - loss: 0.1845 - binary_accuracy: 0.9334\n",
      "Epoch 137/200\n",
      "2165966/2165966 [==============================] - 140s 64us/step - loss: 0.1819 - binary_accuracy: 0.9350\n",
      "Epoch 138/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.1819 - binary_accuracy: 0.9348\n",
      "Epoch 139/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.1873 - binary_accuracy: 0.9316\n",
      "Epoch 140/200\n",
      "2165966/2165966 [==============================] - 140s 65us/step - loss: 0.2015 - binary_accuracy: 0.9240\n",
      "Epoch 141/200\n",
      "2165966/2165966 [==============================] - 140s 65us/step - loss: 0.2140 - binary_accuracy: 0.9194\n",
      "Epoch 142/200\n",
      "2165966/2165966 [==============================] - 140s 65us/step - loss: 0.1996 - binary_accuracy: 0.9255\n",
      "Epoch 143/200\n",
      "2165966/2165966 [==============================] - 140s 65us/step - loss: 0.1828 - binary_accuracy: 0.9323\n",
      "Epoch 144/200\n",
      "2165966/2165966 [==============================] - 142s 65us/step - loss: 0.1885 - binary_accuracy: 0.9302\n",
      "Epoch 145/200\n",
      "2165966/2165966 [==============================] - 140s 65us/step - loss: 0.1834 - binary_accuracy: 0.9335\n",
      "Epoch 146/200\n",
      "2165966/2165966 [==============================] - 140s 65us/step - loss: 0.1783 - binary_accuracy: 0.9354\n",
      "Epoch 147/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.1756 - binary_accuracy: 0.9368\n",
      "Epoch 148/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.1753 - binary_accuracy: 0.9376\n",
      "Epoch 149/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.1824 - binary_accuracy: 0.9350\n",
      "Epoch 150/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.1787 - binary_accuracy: 0.9363\n",
      "Epoch 151/200\n",
      "2165966/2165966 [==============================] - 142s 66us/step - loss: 0.1874 - binary_accuracy: 0.9308\n",
      "Epoch 152/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.1824 - binary_accuracy: 0.9336\n",
      "Epoch 153/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.1888 - binary_accuracy: 0.9300\n",
      "Epoch 154/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.1971 - binary_accuracy: 0.9237\n",
      "Epoch 155/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.1847 - binary_accuracy: 0.9330\n",
      "Epoch 156/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.1781 - binary_accuracy: 0.9364\n",
      "Epoch 157/200\n",
      "2165966/2165966 [==============================] - 141s 65us/step - loss: 0.1779 - binary_accuracy: 0.9371\n",
      "Epoch 158/200\n",
      "2165966/2165966 [==============================] - 142s 66us/step - loss: 0.1771 - binary_accuracy: 0.9367\n",
      "Epoch 159/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.1872 - binary_accuracy: 0.9313\n",
      "Epoch 160/200\n",
      "2165966/2165966 [==============================] - 146s 67us/step - loss: 0.1889 - binary_accuracy: 0.9310\n",
      "Epoch 161/200\n",
      "2165966/2165966 [==============================] - 150s 69us/step - loss: 0.1827 - binary_accuracy: 0.9319\n",
      "Epoch 162/200\n",
      "2165966/2165966 [==============================] - 144s 66us/step - loss: 0.1841 - binary_accuracy: 0.9314\n",
      "Epoch 163/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.1770 - binary_accuracy: 0.9352\n",
      "Epoch 164/200\n",
      "2165966/2165966 [==============================] - 146s 67us/step - loss: 0.1815 - binary_accuracy: 0.9356\n",
      "Epoch 165/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.1780 - binary_accuracy: 0.9371\n",
      "Epoch 166/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.1847 - binary_accuracy: 0.9326\n",
      "Epoch 167/200\n",
      "2165966/2165966 [==============================] - 144s 66us/step - loss: 0.1804 - binary_accuracy: 0.9354\n",
      "Epoch 168/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.1914 - binary_accuracy: 0.9296\n",
      "Epoch 169/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.1850 - binary_accuracy: 0.9316\n",
      "Epoch 170/200\n",
      "2165966/2165966 [==============================] - 144s 67us/step - loss: 0.1928 - binary_accuracy: 0.9265\n",
      "Epoch 171/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.1880 - binary_accuracy: 0.9298\n",
      "Epoch 172/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.1768 - binary_accuracy: 0.9356\n",
      "Epoch 173/200\n",
      "2165966/2165966 [==============================] - 143s 66us/step - loss: 0.1752 - binary_accuracy: 0.9356\n",
      "Epoch 174/200\n",
      "2165966/2165966 [==============================] - 144s 66us/step - loss: 0.1797 - binary_accuracy: 0.9331\n",
      "Epoch 175/200\n",
      "2165966/2165966 [==============================] - 144s 66us/step - loss: 0.1789 - binary_accuracy: 0.9336\n",
      "Epoch 176/200\n",
      "2165966/2165966 [==============================] - 146s 67us/step - loss: 0.1804 - binary_accuracy: 0.9338\n",
      "Epoch 177/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.1767 - binary_accuracy: 0.9374\n",
      "Epoch 178/200\n",
      "2165966/2165966 [==============================] - 144s 66us/step - loss: 0.1770 - binary_accuracy: 0.9365\n",
      "Epoch 179/200\n",
      "2165966/2165966 [==============================] - 144s 66us/step - loss: 0.1781 - binary_accuracy: 0.9356\n",
      "Epoch 180/200\n",
      "2165966/2165966 [==============================] - 144s 67us/step - loss: 0.1795 - binary_accuracy: 0.9353\n",
      "Epoch 181/200\n",
      "2165966/2165966 [==============================] - 144s 66us/step - loss: 0.1796 - binary_accuracy: 0.9347\n",
      "Epoch 182/200\n",
      "2165966/2165966 [==============================] - 144s 67us/step - loss: 0.1788 - binary_accuracy: 0.9354\n",
      "Epoch 183/200\n",
      "2165966/2165966 [==============================] - 146s 67us/step - loss: 0.1800 - binary_accuracy: 0.9355\n",
      "Epoch 184/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.1743 - binary_accuracy: 0.9385\n",
      "Epoch 185/200\n",
      "2165966/2165966 [==============================] - 144s 67us/step - loss: 0.1753 - binary_accuracy: 0.9376\n",
      "Epoch 186/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.1784 - binary_accuracy: 0.9350\n",
      "Epoch 187/200\n",
      "2165966/2165966 [==============================] - 145s 67us/step - loss: 0.1730 - binary_accuracy: 0.9381\n",
      "Epoch 188/200\n",
      "2165966/2165966 [==============================] - 146s 67us/step - loss: 0.1788 - binary_accuracy: 0.9358\n",
      "Epoch 189/200\n",
      "2165966/2165966 [==============================] - 148s 68us/step - loss: 0.1754 - binary_accuracy: 0.9354\n",
      "Epoch 190/200\n",
      "2165966/2165966 [==============================] - 146s 67us/step - loss: 0.1797 - binary_accuracy: 0.9317\n",
      "Epoch 191/200\n",
      "2165966/2165966 [==============================] - 146s 67us/step - loss: 0.1834 - binary_accuracy: 0.9268\n",
      "Epoch 192/200\n",
      "2165966/2165966 [==============================] - 148s 68us/step - loss: 0.1861 - binary_accuracy: 0.9296\n",
      "Epoch 193/200\n",
      "2165966/2165966 [==============================] - 146s 67us/step - loss: 0.1728 - binary_accuracy: 0.9368\n",
      "Epoch 194/200\n",
      "2165966/2165966 [==============================] - 148s 68us/step - loss: 0.1770 - binary_accuracy: 0.9357\n",
      "Epoch 195/200\n",
      "2165966/2165966 [==============================] - 146s 68us/step - loss: 0.1819 - binary_accuracy: 0.9326\n",
      "Epoch 196/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.1770 - binary_accuracy: 0.93380s - loss: 0.1770 - binary_accuracy: 0.93\n",
      "Epoch 197/200\n",
      "2165966/2165966 [==============================] - 146s 68us/step - loss: 0.1729 - binary_accuracy: 0.9367\n",
      "Epoch 198/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.1813 - binary_accuracy: 0.9344\n",
      "Epoch 199/200\n",
      "2165966/2165966 [==============================] - 147s 68us/step - loss: 0.1715 - binary_accuracy: 0.9377\n",
      "Epoch 200/200\n",
      "2165966/2165966 [==============================] - 148s 68us/step - loss: 0.1669 - binary_accuracy: 0.9396\n"
     ]
    }
   ],
   "source": [
    "train_x = train.drop('y',1)\n",
    "train_y = train['y']\n",
    "test_x1 = test1.drop('y',1)\n",
    "test_y1 = test1['y']\n",
    "test_x2 = test2.drop('y',1)\n",
    "test_y2 = test2['y']\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=95, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "history = model.fit(train_x, train_y, epochs = 200, verbose=1)\n",
    "scores1 = model.evaluate(test_x1, test_y1, verbose=0)\n",
    "scores2 = model.evaluate(test_x2, test_y2, verbose=0)\n",
    "\n",
    "model.save('my_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEdCAYAAAC2d5g4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fe0188b9b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_history(history,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22088213904726997, 0.95098958333333339]\n",
      "[2.5459738693695781, 0.8097443509393335]\n"
     ]
    }
   ],
   "source": [
    "print(scores1)\n",
    "print(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    " \n",
    "    \n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "\n",
    "    model = load_model('my_model.h5')\n",
    "    scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "    print(scores)\n",
    "    \n",
    "    del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
